{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 暫時用隨機數生成嵌入向量進行測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8167249  0.00189662 0.7692364  ... 0.33065252 0.42364289 0.76526424]\n",
      " [0.40393225 0.94905566 0.31760222 ... 0.32359013 0.63768525 0.96624867]\n",
      " [0.30071281 0.27432618 0.55649584 ... 0.72672837 0.34417059 0.23525048]\n",
      " ...\n",
      " [0.18201977 0.63673909 0.66854363 ... 0.37664562 0.01708142 0.51996078]\n",
      " [0.81234222 0.61392967 0.28730779 ... 0.67729375 0.43603341 0.23344004]\n",
      " [0.57664726 0.92528192 0.13464751 ... 0.97966627 0.01688898 0.79387565]]\n",
      "(30, 768)\n",
      "嵌入向量文件已生成！\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 隨機生成新聞嵌入\n",
    "news_embeddings = np.random.rand(30, 768)  # 30 條新聞，嵌入維度 768\n",
    "print(news_embeddings)\n",
    "print(news_embeddings.shape)\n",
    "np.save('news_embeddings.npy', news_embeddings)\n",
    "\n",
    "# 隨機生成實體嵌入\n",
    "entity_embeddings = np.random.rand(50, 768)  # 50 個實體節點\n",
    "np.save('entity_embeddings.npy', entity_embeddings)\n",
    "\n",
    "# 隨機生成主題嵌入\n",
    "topic_embeddings = np.random.rand(5, 768)  # 5 個主題節點\n",
    "np.save('topic_embeddings_5.npy', topic_embeddings)\n",
    "\n",
    "print(\"嵌入向量文件已生成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 暫時沒有關係文件，模擬生成Excel 關係文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "關係文件已成功生成！\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "num_news = 30  # 新聞數\n",
    "num_entities = 50  # 實體數\n",
    "num_topics = 5  # 主題數\n",
    "\n",
    "# 生成新聞對實體的關係文件\n",
    "news2entity = []\n",
    "for news_id in range(num_news):\n",
    "    # 每條新聞至少連接 2 個實體\n",
    "    connected_entities = np.random.choice(range(num_entities), size=np.random.randint(2, 5), replace=False)\n",
    "    for entity_id in connected_entities:\n",
    "        news2entity.append({'news_id': news_id, 'entity_id': entity_id})\n",
    "\n",
    "# 確保每個實體至少被連接一次\n",
    "for entity_id in range(num_entities):\n",
    "    if entity_id not in [relation['entity_id'] for relation in news2entity]:\n",
    "        news_id = np.random.choice(range(num_news))\n",
    "        news2entity.append({'news_id': news_id, 'entity_id': entity_id})\n",
    "\n",
    "news2entity_df = pd.DataFrame(news2entity)\n",
    "news2entity_df.to_excel('/home/blueee/LESS4FD/Data/Random_test/graph/edges/news2entity.xlsx', index=False)\n",
    "\n",
    "# 生成新聞對主題的關係文件\n",
    "news2topic = []\n",
    "for news_id in range(num_news):\n",
    "    # 每條新聞隨機連接 1 個主題\n",
    "    topic_id = np.random.choice(range(num_topics))\n",
    "    news2topic.append({'news_id': news_id, 'topic_id': topic_id})\n",
    "\n",
    "# 確保每個主題至少被連接一次\n",
    "for topic_id in range(num_topics):\n",
    "    if topic_id not in [relation['topic_id'] for relation in news2topic]:\n",
    "        news_id = np.random.choice(range(num_news))\n",
    "        news2topic.append({'news_id': news_id, 'topic_id': topic_id})\n",
    "\n",
    "news2topic_df = pd.DataFrame(news2topic)\n",
    "news2topic_df.to_excel('/home/blueee/LESS4FD/Data/Random_test/graph/edges/news2topic_{}.xlsx'.format(num_topics), index=False)\n",
    "\n",
    "print(\"關係文件已生成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根據已經存在的所有excel檔案生成三個 _index.npy檔，內容是字典(不重複的)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news_index.npy 已生成\n",
      "entity_index.npy 已生成\n",
      "topic_index_5.npy 已生成\n",
      "news_index: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29}\n",
      "entity_index: {33: 0, 30: 1, 23: 2, 15: 3, 7: 4, 17: 5, 22: 6, 14: 7, 12: 8, 27: 9, 40: 10, 45: 11, 9: 12, 39: 13, 43: 14, 28: 15, 36: 16, 5: 17, 48: 18, 1: 19, 21: 20, 49: 21, 34: 22, 44: 23, 0: 24, 6: 25, 20: 26, 24: 27, 3: 28, 35: 29, 26: 30, 11: 31, 32: 32, 31: 33, 25: 34, 2: 35, 46: 36, 18: 37, 10: 38, 8: 39, 47: 40, 37: 41, 4: 42, 13: 43, 16: 44, 19: 45, 29: 46, 38: 47, 41: 48, 42: 49}\n",
      "topic_index: {4: 0, 3: 1, 1: 2, 0: 3, 2: 4}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_indices(dataset, num_topics):\n",
    "    \n",
    "    news_df = pd.read_excel(f'/home/blueee/LESS4FD/Data/{dataset}/news_final.xlsx')\n",
    "    news_ids = news_df['news_id'].tolist()\n",
    "    news_index = {int(news_id): idx for idx, news_id in enumerate(news_ids)}\n",
    "    np.save(f'/home/blueee/LESS4FD/Data/{dataset}/graph/nodes/news_index.npy', news_index)\n",
    "    print(\"news_index.npy 已生成\")\n",
    "\n",
    "    \n",
    "    entity_df = pd.read_excel(f'/home/blueee/LESS4FD/Data/{dataset}/graph/edges/news2entity.xlsx')\n",
    "    entity_ids = entity_df['entity_id'].unique().tolist()\n",
    "    entity_index = {int(entity_id): idx for idx, entity_id in enumerate(entity_ids)}\n",
    "    np.save(f'/home/blueee/LESS4FD/Data/{dataset}/graph/nodes/entity_index.npy', entity_index)\n",
    "    print(\"entity_index.npy 已生成\")\n",
    "\n",
    "    \n",
    "    topic_df = pd.read_excel(f'/home/blueee/LESS4FD/Data/{dataset}/graph/edges/news2topic_{num_topics}.xlsx')\n",
    "    topic_ids = topic_df['topic_id'].unique().tolist()\n",
    "    topic_index = {int(topic_id): idx for idx, topic_id in enumerate(topic_ids)}\n",
    "    np.save(f'/home/blueee/LESS4FD/Data/{dataset}/graph/nodes/topic_index_{num_topics}.npy', topic_index)\n",
    "    print(f\"topic_index_{num_topics}.npy 已生成\")\n",
    "\n",
    "    print(f\"news_index: {news_index}\")\n",
    "    print(f\"entity_index: {entity_index}\")\n",
    "    print(f\"topic_index: {topic_index}\")\n",
    "\n",
    "\n",
    "dataset = \"Random_test\"  # 數據集\n",
    "num_topics = 5  # 主題數\n",
    "generate_indices(dataset, num_topics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成全局索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global index dictionary: {'news_0': 0, 'news_1': 1, 'news_2': 2, 'news_3': 3, 'news_4': 4, 'news_5': 5, 'news_6': 6, 'news_7': 7, 'news_8': 8, 'news_9': 9, 'news_10': 10, 'news_11': 11, 'news_12': 12, 'news_13': 13, 'news_14': 14, 'news_15': 15, 'news_16': 16, 'news_17': 17, 'news_18': 18, 'news_19': 19, 'news_20': 20, 'news_21': 21, 'news_22': 22, 'news_23': 23, 'news_24': 24, 'news_25': 25, 'news_26': 26, 'news_27': 27, 'news_28': 28, 'news_29': 29, 'entity_33': 30, 'entity_30': 31, 'entity_23': 32, 'entity_15': 33, 'entity_7': 34, 'entity_17': 35, 'entity_22': 36, 'entity_14': 37, 'entity_12': 38, 'entity_27': 39, 'entity_40': 40, 'entity_45': 41, 'entity_9': 42, 'entity_39': 43, 'entity_43': 44, 'entity_28': 45, 'entity_36': 46, 'entity_5': 47, 'entity_48': 48, 'entity_1': 49, 'entity_21': 50, 'entity_49': 51, 'entity_34': 52, 'entity_44': 53, 'entity_0': 54, 'entity_6': 55, 'entity_20': 56, 'entity_24': 57, 'entity_3': 58, 'entity_35': 59, 'entity_26': 60, 'entity_11': 61, 'entity_32': 62, 'entity_31': 63, 'entity_25': 64, 'entity_2': 65, 'entity_46': 66, 'entity_18': 67, 'entity_10': 68, 'entity_8': 69, 'entity_47': 70, 'entity_37': 71, 'entity_4': 72, 'entity_13': 73, 'entity_16': 74, 'entity_19': 75, 'entity_29': 76, 'entity_38': 77, 'entity_41': 78, 'entity_42': 79, 'topic_4': 80, 'topic_3': 81, 'topic_1': 82, 'topic_0': 83, 'topic_2': 84}\n",
      "Total nodes: 85\n",
      "global_index_5.npy 已生成！\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataset = \"Random_test\"  # 數據集\n",
    "num_topics = 5  # 主題數\n",
    "news_index = np.load(f'/home/blueee/LESS4FD/Data/{dataset}/graph/nodes/news_index.npy', allow_pickle=True).item()\n",
    "entity_index = np.load(f'/home/blueee/LESS4FD/Data/{dataset}/graph/nodes/entity_index.npy', allow_pickle=True).item()\n",
    "topic_index = np.load(f'/home/blueee/LESS4FD/Data/{dataset}/graph/nodes/topic_index_{num_topics}.npy', allow_pickle=True).item()\n",
    "\n",
    "global_index = {}\n",
    "\n",
    "# 1. 添加新聞節點到全局索引\n",
    "current_index = 0\n",
    "for news_id, local_index in news_index.items():\n",
    "    global_index[f\"news_{news_id}\"] = current_index\n",
    "    current_index += 1  # current_index就是全局索引的值，此值唯一，每添加一個節點，current_index就加1\n",
    "\n",
    "# 2. 添加實體節點到全局索引\n",
    "for entity_id, local_index in entity_index.items():\n",
    "    global_index[f\"entity_{entity_id}\"] = current_index\n",
    "    current_index += 1\n",
    "\n",
    "# 3. 添加主題節點到全局索引\n",
    "for topic_id, local_index in topic_index.items():\n",
    "    global_index[f\"topic_{topic_id}\"] = current_index\n",
    "    current_index += 1\n",
    "\n",
    "\n",
    "print(f\"Global index dictionary: {global_index}\")\n",
    "print(f\"Total nodes: {len(global_index)}\")  # 應該等於 (新聞數) + (entity數) + (topic數)\n",
    "\n",
    "np.save(f'/home/blueee/LESS4FD/Data/{dataset}/graph/nodes/global_index_{num_topics}.npy', global_index)\n",
    "print(f\"global_index_{num_topics}.npy 已生成！\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "less4fd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
